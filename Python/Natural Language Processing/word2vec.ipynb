{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"colab":{"name":"word2vec.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"JSkiHNEPKi0D"},"source":["%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from IPython import display"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"XWifzNxzKq1S","executionInfo":{"status":"ok","timestamp":1618306701848,"user_tz":-480,"elapsed":14245,"user":{"displayName":"rex tsou","photoUrl":"","userId":"15448100786581130255"}},"outputId":"32423153-3833-4438-9980-33bb3fc732d1"},"source":["import os\n","os.environ['TRIDENT_BACKEND'] = 'pytorch'\n","\n","!pip uninstall tridentx\n","!pip install tridentx --upgrade\n","\n","from trident import *\n","from typing import Optional,List,Tuple\n","import locale\n","import datetime\n","import tqdm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[33mWARNING: Skipping tridentx as it is not installed.\u001b[0m\n","Collecting tridentx\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/28/4f2063ee2f3c5b667e75d0613cc883b60aafb63c030ebb60440da4fdb01c/tridentx-0.7.2-py3-none-any.whl (694kB)\n","\u001b[K     |████████████████████████████████| 696kB 17.0MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from tridentx) (4.41.1)\n","Requirement already satisfied, skipping upgrade: scikit-image>=0.15 in /usr/local/lib/python3.7/dist-packages (from tridentx) (0.16.2)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from tridentx) (54.2.0)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from tridentx) (2.23.0)\n","Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from tridentx) (7.1.2)\n","Requirement already satisfied, skipping upgrade: matplotlib>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from tridentx) (3.2.2)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from tridentx) (3.13)\n","Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.7/dist-packages (from tridentx) (4.1.2.30)\n","Requirement already satisfied, skipping upgrade: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from tridentx) (1.19.5)\n","Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tridentx) (1.15.0)\n","Requirement already satisfied, skipping upgrade: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from tridentx) (1.4.1)\n","Requirement already satisfied, skipping upgrade: dill>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from tridentx) (0.3.3)\n","Requirement already satisfied, skipping upgrade: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from tridentx) (2.4.1)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from tridentx) (2.10.0)\n","Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx) (1.1.1)\n","Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx) (2.5)\n","Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15->tridentx) (2.4.1)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tridentx) (2.10)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx) (0.10.0)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx) (1.3.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx) (2.4.7)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.2->tridentx) (2.8.1)\n","Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (0.12.0)\n","Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (1.28.0)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (3.12.4)\n","Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (1.8.0)\n","Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (0.4.3)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (1.0.1)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (3.3.4)\n","Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (0.36.2)\n","Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->tridentx) (1.32.0)\n","Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.15->tridentx) (4.4.2)\n","Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->tridentx) (4.2.1)\n","Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->tridentx) (0.2.8)\n","Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->tridentx) (4.7.2)\n","Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->tridentx) (1.3.0)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->tridentx) (3.8.1)\n","Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->tridentx) (0.4.8)\n","Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->tridentx) (3.1.0)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->tridentx) (3.4.1)\n","Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->tridentx) (3.7.4.3)\n","Installing collected packages: tridentx\n","Successfully installed tridentx-0.7.2\n"],"name":"stdout"},{"output_type":"stream","text":["trident 0.7.2\n"],"name":"stderr"},{"output_type":"stream","text":["Using Pytorch backend.\n","Image Data Format: channels_first.\n","Image Channel Order: rgb.\n","Pytorch version:1.8.1+cu101.\n","Automatic Mixed Precision Support:False.\n"],"name":"stdout"},{"output_type":"stream","text":["Opencv version:4.1.2.\n","Pillow version:7.1.2.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/javascript":["window[\"fa577b18-9c3b-11eb-884c-0242ac1c0002\"] = to(\"cpu\");\n","//# sourceURL=js_c2b06c7811"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["window[\"fa58153c-9c3b-11eb-884c-0242ac1c0002\"] = google.colab.to(\"cpu\");\n","//# sourceURL=js_d8b3ca265a"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n","  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"JDa6sMTXjiYG"},"source":["class Word2Vec(Embedding):\n","    \"\"\"中文詞向量\n","        繼承Embedding Layer\n","\n","    \"\"\"\n","    def __init__(self, pretrained=False,locale=None, embedding_dim: Optional[int]= None, num_embeddings: Optional[int] = None, vocabs: Optional[List[str]] = None, padding_idx: Optional[int] = None,\n","                 max_norm: Optional[float] = None, norm_type: float = 2., scale_grad_by_freq: bool = False,\n","                 sparse: bool = False, _weight: Optional[Tensor] = None, filter_index=-1, keep_output: bool = False, name: Optional[str] = None) -> None:\n","\n","        \"\"\"\n","        Py Word2vec结构\n","        \"\"\"\n","        super().__init__(num_embeddings=num_embeddings,embedding_dim=embedding_dim,max_norm=max_norm,norm_type=norm_type,scale_grad_by_freq=scale_grad_by_freq,sparse=sparse,_weight=_weight,filter_index=filter_index, keep_output= keep_output, name=name)\n","        self.locale =ctx.locale\n","        print('locale:', self.locale)\n","\n","        self._vocabs = OrderedDict()\n","        if vocabs is not None:\n","            for k in range(len(vocabs)):\n","                self._vocabs[vocabs[k]] = k\n","\n","    @property\n","    def vocabs(self):\n","       #詞彙表\n","       return self._vocabs\n","\n","\n","    def word2idx(self, word: str):\n","        # 文字轉索引(根據locale處理繁簡轉換)\n","        if self.locale != 'zh_cn' and word in self.tw2cn:\n","            word=self.tw2cn[word]\n","        if word in  self._vocabs :\n","            return self._vocabs[word]\n","        else:\n","            return None\n","\n","    def idx2word(self, index: int):\n","        # 索引轉文字(根據locale處理繁簡轉換)\n","        if index < len(self._vocabs):\n","            word= self._vocabs.key_list[index]\n","            if self.locale != 'zh_cn'and word in self.cn2tw:\n","                word=self.cn2tw[word]\n","            return word\n","        else:\n","            return None\n","\n","    @classmethod\n","    def load(cls):\n","        # 從google drive載入模型\n","        st = datetime.datetime.now()\n","        set_device('cpu')\n","        dirname = os.path.join(get_trident_dir(), 'models')\n","        download_model_from_google_drive('13XZPWh8QhEsC8EdIp1niLtZz0ipatSGC', dirname, 'word2vec_chinese.pth')\n","        recovery_model = load(os.path.join(dirname, 'word2vec_chinese.pth'))\n","        recovery_weight=recovery_model.state_dict()['weight']\n","        shp=int_shape(recovery_weight)\n","\n","        v = cls(pretrained=True,num_embeddings=shp[0], embedding_dim=shp[-1],_weight=recovery_weight,name='word2vec_chinese')\n","        v._vocabs=copy.deepcopy(recovery_model._vocabs)\n","        v.tw2cn =copy.deepcopy(recovery_model.tw2cn)\n","        v.cn2tw = copy.deepcopy(recovery_model.cn2tw)\n","        del recovery_model\n","        v.locale =ctx.locale\n","        v.to(get_device())\n","        et = datetime.datetime.now()\n","        print('total loading time:{0}'.format(et - st))\n","        return v\n","\n","    def find_similar(self,reprt:(str,Tensor), n:int=10,ignore_indexes=None):\n","        #根據文字或是向量查詢空間中最近文字\n","        reprt_idx=None\n","        if ignore_indexes is None:\n","            ignore_indexes=[]\n","        if isinstance(reprt,str):\n","            reprt_idx=self.word2idx(reprt)\n","            ignore_indexes.append(reprt_idx)\n","            reprt = self.weight[reprt_idx].expand_dims(0) if reprt in self._vocabs else None\n","        if is_tensor(reprt):\n","            correlate=element_cosine_distance(reprt,self.weight)[0]\n","            sorted_idxes=argsort(correlate,descending=True)\n","\n","            sorted_idxes=sorted_idxes[:n+len(ignore_indexes)]\n","            \n","            sorted_idxes=to_tensor([idx for idx in sorted_idxes if  idx.item()  not in ignore_indexes]).long()\n","            probs=to_list(correlate[sorted_idxes])[:n]\n","            words=[self.idx2word(idx.item())for idx in sorted_idxes][:n]\n","            return OrderedDict(zip(words,probs))\n","        else:\n","            raise ValueError('Valid reprt should be a word or a tensor .')\n","\n","    def analogy(self,reprt1:(str,Tensor,list),reprt2:(str,Tensor,list),reprt3:(str,Tensor,list),n:int=10):\n","        #類比關係 (男人之於女人等於國王之於皇后)\n","        reprt1_idx=None\n","        reprt2_idx=None\n","        reprt3_idx=None\n","        reprt1_arr = None\n","        reprt2_arr= None\n","        reprt3_arr = None\n","        exclude_list=[]\n","        if isinstance(reprt1,str):\n","            reprt1_idx=self.word2idx(reprt1)\n","            exclude_list.append(reprt1_idx)\n","            reprt1_arr=self.weight[reprt1_idx].expand_dims(0) if reprt1_idx is not None else None\n","        elif isinstance(reprt1,Tensor):\n","            reprt1_arr = reprt1\n","        elif isinstance(reprt1,list):\n","            if isinstance(reprt1[0],str):\n","                reprt1_arr=self.get_words_centroid(*reprt1)\n","                for item in reprt1:\n","                    exclude_list.append(self.word2idx(item))\n","\n","        if isinstance(reprt2, str):\n","            reprt2_idx = self.word2idx(reprt2)\n","            exclude_list.append(reprt2_idx)\n","            reprt2_arr = self.weight[reprt2_idx].expand_dims(0) if reprt2_idx is not None else None\n","        elif isinstance(reprt2, Tensor):\n","            reprt2_arr = reprt2\n","        elif isinstance(reprt2, list):\n","            if isinstance(reprt2[0], str):\n","                reprt2_arr = self.get_words_centroid(*reprt2)\n","                for item in reprt2:\n","                    exclude_list.append(self.word2idx(item))\n","\n","        if isinstance(reprt3,str):\n","            reprt3_idx=self.word2idx(reprt3)\n","            exclude_list.append(reprt3_idx)\n","            reprt3_arr=self.weight[reprt3_idx].expand_dims(0) if reprt3_idx is not None else None\n","        elif isinstance(reprt3,Tensor):\n","            reprt3_arr = reprt3\n","        elif isinstance(reprt3,list):\n","            if isinstance(reprt3[0],str):\n","                reprt3_arr=self.get_words_centroid(*reprt3)\n","                for item in reprt3:\n","                    exclude_list.append(self.word2idx(item))\n","\n","        if reprt1_arr is not None and reprt2_arr is not None and reprt3_arr is not None:\n","            reprt4=reprt2_arr-reprt1_arr+reprt3_arr\n","            return self.find_similar(reprt4,n=n,ignore_indexes=exclude_list)\n","        else:\n","            not_find=[]\n","            if reprt1_arr is None:\n","                not_find.append(reprt1)\n","            if reprt2_arr is None:\n","                not_find.append(reprt2)\n","            if reprt3_arr is None:\n","                not_find.append(reprt3)\n","            raise ValueError(' ,'.join(not_find)+' was not in vocabs.')\n","        \n","    def get_words_centroid(self,*args):\n","        #取得數個文字的向量均值\n","        centroid=0\n","        for arg in args:\n","            reprt_idx=self.word2idx(arg)\n","            if reprt_idx is not None:\n","                centroid+=self.weight[reprt_idx].expand_dims(0) if reprt_idx is not None else None\n","        return centroid/len(args)\n","\n","    def get_words_vector(self, word):\n","        #取得單一文字的向量\n","        reprt_idx=self.word2idx(word)\n","        if reprt_idx is not None:\n","            return self.weight[reprt_idx].expand_dims(0) if reprt_idx is not None else None\n","        return None\n","\n","    def get_enumerators(self, *args,negative_case=None,n=10,exclude_samples=True):\n","        #取得整體距離輸入案例最接近，但是離負案例最遠(negative_case)的文字列表\n","        positive_correlate=0\n","        negative_correlate=0\n","        exclude_list=[]\n","        for arg in args:\n","            positive_correlate +=element_cosine_distance(self.get_words_vector(arg), self.weight)[0]\n","\n","        correlate=positive_correlate\n","        if negative_case is None:\n","            pass\n","        else:\n","            if isinstance(negative_case,str):\n","                negative_case=[negative_case]\n","            if isinstance(negative_case,(list,tuple)):\n","                for arg in negative_case:\n","                    negative_correlate += element_cosine_distance(self.get_words_vector(arg), self.weight)[0]\n","                correlate=positive_correlate-negative_correlate\n","        sorted_idxes = argsort(correlate, descending=True)\n","        sorted_idxes = sorted_idxes[:n + len(exclude_list)]\n","        sorted_idxes = to_tensor([idx for idx in sorted_idxes if idx.item() not in exclude_list]).long()\n","        probs = to_list(correlate[sorted_idxes])[:n]\n","        words = [self.idx2word(idx.item()) for idx in sorted_idxes][:n]\n","        return OrderedDict(zip(words, probs))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9D5t9Os8jk6E","colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"f8d53a53-9eff-4cea-fcac-95c396ec689a"},"source":["w2v=Word2Vec.load()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["window[\"af77098c-9c3c-11eb-884c-0242ac1c0002\"] = to(\"cpu\");\n","//# sourceURL=js_ddd7c590e9"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["window[\"af7752ca-9c3c-11eb-884c-0242ac1c0002\"] = google.colab.to(\"cpu\");\n","//# sourceURL=js_d3d8f7f2f0"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"6r05N7a-jrub","executionInfo":{"status":"error","timestamp":1618316129669,"user_tz":-480,"elapsed":966,"user":{"displayName":"rex tsou","photoUrl":"","userId":"15448100786581130255"}},"outputId":"978503d9-d0d7-4f1f-d7e8-83c3bddb5cad","colab":{"base_uri":"https://localhost:8080/","height":167}},"source":["w2v.analogy('男人','國王','女人')"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b735f80e69cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'男人'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'國王'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'女人'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'w2v' is not defined"]}]},{"cell_type":"code","metadata":{"id":"zyo0-JJtjyOu"},"source":["w2v.analogy('男人','工程師','女人')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ErDUgqpAjzEK"},"source":["w2v.analogy('張惠妹','阿妹','周杰倫')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f5qq7-c6jzOH"},"source":["w2v.analogy('雙子座',['花心','聰明'],'金牛座')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nbn5By8Ajzg8"},"source":["w2v.analogy('黃曉明','angelababy','胡歌')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DMMm3Pnjzot"},"source":["w2v.analogy('基金','贖回','期貨')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RaZai1fjztX"},"source":["w2v.analogy('鹵肉飯','米飯','牛肉面')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rbI_3aiDjzxr"},"source":["w2v.get_enumerators('波蘭','捷克',n=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H050noHxj_A-"},"source":["w2v.get_enumerators('美金','人民幣',n=20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"syMMtvZdj_D7"},"source":["w2v.get_enumerators('蘋果','香蕉',n=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lB39LegtkI1D"},"source":["w2v.get_enumerators('蘋果','香蕉',negative_case='微軟',n=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BtuhuidskI7L"},"source":["resuts=w2v.get_enumerators('美國','中國','泰國','德國',n=10,exclude_samples='全球')\n","for key in resuts.key_list:\n","    print(key, w2v.analogy(['國家','澳大利亞'], ['首都','坎培拉'], key, n=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WeBsiqR2kI-o"},"source":["resuts=w2v.get_enumerators('小米','富士康','格力','百度','企業',negative_case='代工廠',n=10)\n","for key in resuts.key_list:\n","    print(key, w2v.analogy('騰訊', '馬化騰', key, n=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0OjinlAkJBu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_KEVxwUZj_Gj"},"source":["stars = ['白羊座', '金牛座','雙子座', '巨蟹座', '獅子座', '處女座', '天秤座', '天蝎座', '射手座', '摩羯座', '水瓶座',\n","             '雙魚座']\n","for key in stars:\n","    print(key, w2v.analogy(['雙子座'],['花心'], key, n=5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Clo1zG2kUcd"},"source":["stars = ['白羊座', '金牛座','雙子座', '巨蟹座', '獅子座', '處女座', '天秤座', '天蝎座', '射手座', '摩羯座', '水瓶座',\n","             '雙魚座']\n","for key in stars:\n","    print(key, w2v.analogy(['雙子座','星座'],['花心','聰明','喜新厭舊','性格'], key, n=5))"],"execution_count":null,"outputs":[]}]}